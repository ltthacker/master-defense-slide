\section{Proposed method - \gls{propose}}

\begin{frame}{Task selection as a Multi Armed Bandit (MAB) problem}
    \begin{block}{Given}
        \begin{itemize}
            \item A $K$-task MTO problem
        \end{itemize}
    \end{block}
    \begin{mydef}[Action]
        For a task $k$, there are $K-1$ actions of choosing $k'$ such that $k' \in \{1, \ldots, K\} \text{ and } k' \ne k$.
        \label{def:action}
    \end{mydef}
    \begin{mydef}[Reward]
        After task $k'$ is selected to be combined with task $k$, the reward of choosing that action is defined as 
        \begin{equation}
            reward = \left\{
                \begin{array}{ll}
                    1 \text{ if } f_k(c) < f_k(p) , \exists p \in P_k \\\
                    0 \text{ otherwise}.
                \end{array}
              \right.
        \end{equation}
        where $c$ is the offspring generated by the reproduction procedure and $f_k(.)$ is the fitness function of the $k^{th}$ task.
        \label{def:reward}
    \end{mydef}
\end{frame}

\begin{frame}{UCB function to solve MAB}
    \begin{block}{Property of reward function}
        \begin{itemize}
            \item Reward takes two value $0$ or $1$ $\rightarrow$ reward distribution is generated from an unknown Bernoulli distribution.
            \item From \footfullcite{lattimore2020bandit}, use KL-UCB to solve.
        \end{itemize}
    \end{block}
    \begin{block}{KL-UCB function}
        \begin{equation}
            k' = \underset{j}{\text{argmin }} \mu(j) + \frac{1 + t \times log^2(t) }{N(j)}
            \label{eq:klucb}
        \end{equation}
        where 
        \begin{itemize}
            \item $\mu(j)$ is the mean of reward when task $j$ is chosen
            \item $N(j)$ is the number of times task $j$ is chosen
            \item $t$ is the total number of actions chosen.
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{\gls{propose}, optimize 1 task, 1 generation}
    \begin{algorithm}[H]
        \fontsize{6pt}{10}\selectfont
        \caption{\fontsize{6pt}{10}\selectfont\gls{propose} on each generation of $k^{th}$ task}
        \begin{algorithmic}[1]
            \State Initialize $P^{(c)}_k=\emptyset$;
            \While{number of offspring $< N$}
                \State Randomly select $p_a$ from $P_k$;
                \If{$rand(0, 1) < rmp$}
                    \State Choose $k'$ using Formula \eqref{eq:klucb};
                    \State Randomly select $p_b$ from $P_{k'}$;
                    \State $c = $ \emph{Inter-task crossover} between  $p_a$ and $p_b$;
                \Else
                    \State Sample $p_b$ from $P_k$;
                    \State $c = $ \emph{Intra-task crossover} between  $p_a$ and $p_b$;
                \EndIf
                \State $c = mutate(c)$;
                \State Evaluate offspring $c$;
                \State Update estimation $\mu(k')$ and $N(k')$ for \gls{klucb} if $c$ generated by inter-task crossover;
                \State $P_k^{(c)} = P_c \cup \{c\}$;
            \EndWhile
            \State $P_k \leftarrow$ Select $N$ best individuals from $P_k \cup P_k^{(c)}$ to form the next-population of task $k$;
        \end{algorithmic}
    \end{algorithm}
\end{frame}


\begin{frame}{\gls{propose}, proposed structure for manytasking}
    \begin{algorithm}[H]
        \caption{Pseudo code of \gls{propose}}
        \begin{algorithmic}[1]
            \For{$k \in \{1, \ldots, K\}$}
                \State Randomly sample $N$ individuals to form subpopulation $P_k$;
                \State Evaluate individual in $P_k$ for task $k$ only;
            \EndFor
            \While{stopping conditions are not satisfied}
                \For{$k \in random\_permutation({1, \ldots, K})$}
                    \State Invoke Algorithm 1 for task $k$;
                \EndFor
            \EndWhile
        \end{algorithmic}
    \end{algorithm}
\end{frame}
